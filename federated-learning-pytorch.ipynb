{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Importing Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nfrom torchvision.datasets import MNIST\nfrom torch.utils.data import random_split, DataLoader\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [5, 5]","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Dataset"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"train_dataset = MNIST('/kaggle/working', train=True, download=True, transform=transforms.ToTensor())\ntest_dataset = MNIST('/kaggle/working', train=False, download=True, transform=transforms.ToTensor())\n\ntrain_dataset, dev_dataset = random_split(train_dataset, [int(len(train_dataset) * 0.83), int(len(train_dataset) * 0.17)])","execution_count":2,"outputs":[{"output_type":"stream","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /kaggle/working/MNIST/raw/train-images-idx3-ubyte.gz\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17486143f00145ff8beb8425928b7b3b"}},"metadata":{}},{"output_type":"stream","text":"Extracting /kaggle/working/MNIST/raw/train-images-idx3-ubyte.gz to /kaggle/working/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /kaggle/working/MNIST/raw/train-labels-idx1-ubyte.gz\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c4764bcc15347ebabb58de5d2a725b5"}},"metadata":{}},{"output_type":"stream","text":"Extracting /kaggle/working/MNIST/raw/train-labels-idx1-ubyte.gz to /kaggle/working/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /kaggle/working/MNIST/raw/t10k-images-idx3-ubyte.gz\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92f543524b0046189b27d890e1baf436"}},"metadata":{}},{"output_type":"stream","text":"Extracting /kaggle/working/MNIST/raw/t10k-images-idx3-ubyte.gz to /kaggle/working/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /kaggle/working/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0b766f9d3d646cc95446ff4f8a89caa"}},"metadata":{}},{"output_type":"stream","text":"Extracting /kaggle/working/MNIST/raw/t10k-labels-idx1-ubyte.gz to /kaggle/working/MNIST/raw\nProcessing...\nDone!\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/conda-bld/pytorch_1591914880026/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"### Define Notebook Constants"},{"metadata":{"trusted":true},"cell_type":"code","source":"total_train_size = len(train_dataset)\ntotal_test_size = len(test_dataset)\ntotal_dev_size = len(dev_dataset)\n\nclasses = 10\ninput_dim = 784\n\nnum_clients = 8\nrounds = 30\nbatch_size = 128\nepochs_per_client = 3\nlearning_rate = 2e-2","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_train_size, total_dev_size, total_test_size","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"(49800, 10200, 10000)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Define utilities for GPU support"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_device():\n    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\ndef to_device(data, device):\n    if isinstance(data, (list, tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader(DataLoader):\n        def __init__(self, dl, device):\n            self.dl = dl\n            self.device = device\n\n        def __iter__(self):\n            for batch in self.dl:\n                yield to_device(batch, self.device)\n\n        def __len__(self):\n            return len(self.dl)\n\ndevice = get_device()","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define FederatedNet class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class FederatedNet(torch.nn.Module):    \n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 20, 7)\n        self.conv2 = torch.nn.Conv2d(20, 40, 7)\n        self.maxpool = torch.nn.MaxPool2d(2, 2)\n        self.flatten = torch.nn.Flatten()\n        self.linear = torch.nn.Linear(2560, 10)\n        self.non_linearity = torch.nn.functional.relu\n        self.track_layers = {'conv1': self.conv1, 'conv2': self.conv2, 'linear': self.linear}\n    \n    def forward(self, x_batch):\n        out = self.conv1(x_batch)\n        out = self.non_linearity(out)\n        out = self.conv2(out)\n        out = self.non_linearity(out)\n        out = self.maxpool(out)\n        out = self.flatten(out)\n        out = self.linear(out)\n        return out\n    \n    def get_track_layers(self):\n        return self.track_layers\n    \n    def apply_parameters(self, parameters_dict):\n        with torch.no_grad():\n            for layer_name in parameters_dict:\n                self.track_layers[layer_name].weight.data *= 0\n                self.track_layers[layer_name].bias.data *= 0\n                self.track_layers[layer_name].weight.data += parameters_dict[layer_name]['weight']\n                self.track_layers[layer_name].bias.data += parameters_dict[layer_name]['bias']\n    \n    def get_parameters(self):\n        parameters_dict = dict()\n        for layer_name in self.track_layers:\n            parameters_dict[layer_name] = {\n                'weight': self.track_layers[layer_name].weight.data, \n                'bias': self.track_layers[layer_name].bias.data\n            }\n        return parameters_dict\n    \n    def batch_accuracy(self, outputs, labels):\n        with torch.no_grad():\n            _, predictions = torch.max(outputs, dim=1)\n            return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n    \n    def _process_batch(self, batch):\n        images, labels = batch\n        outputs = self(images)\n        loss = torch.nn.functional.cross_entropy(outputs, labels)\n        accuracy = self.batch_accuracy(outputs, labels)\n        return (loss, accuracy)\n    \n    def fit(self, dataset, epochs, lr, batch_size=128, opt=torch.optim.SGD):\n        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size, shuffle=True), device)\n        optimizer = opt(self.parameters(), lr)\n        history = []\n        for epoch in range(epochs):\n            losses = []\n            accs = []\n            for batch in dataloader:\n                loss, acc = self._process_batch(batch)\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n                loss.detach()\n                losses.append(loss)\n                accs.append(acc)\n            avg_loss = torch.stack(losses).mean().item()\n            avg_acc = torch.stack(accs).mean().item()\n            history.append((avg_loss, avg_acc))\n        return history\n    \n    def evaluate(self, dataset, batch_size=128):\n        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size), device)\n        losses = []\n        accs = []\n        with torch.no_grad():\n            for batch in dataloader:\n                loss, acc = self._process_batch(batch)\n                losses.append(loss)\n                accs.append(acc)\n        avg_loss = torch.stack(losses).mean().item()\n        avg_acc = torch.stack(accs).mean().item()\n        return (avg_loss, avg_acc)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define Client class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Client:\n    def __init__(self, client_id, dataset):\n        self.client_id = client_id\n        self.dataset = dataset\n    \n    def get_dataset_size(self):\n        return len(self.dataset)\n    \n    def get_client_id(self):\n        return self.client_id\n    \n    def train(self, parameters_dict):\n        net = to_device(FederatedNet(), device)\n        net.apply_parameters(parameters_dict)\n        train_history = net.fit(self.dataset, epochs_per_client, learning_rate, batch_size)\n        print('{}: Loss = {}, Accuracy = {}'.format(self.client_id, round(train_history[-1][0], 4), round(train_history[-1][1], 4)))\n        return net.get_parameters()","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Setup clients "},{"metadata":{"trusted":true},"cell_type":"code","source":"examples_per_client = total_train_size // num_clients\nclient_datasets = random_split(train_dataset, [min(i + examples_per_client, \n           total_train_size) - i for i in range(0, total_train_size, examples_per_client)])\nclients = [Client('client_' + str(i), client_datasets[i]) for i in range(num_clients)]","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Start server"},{"metadata":{"trusted":true},"cell_type":"code","source":"global_net = to_device(FederatedNet(), device)\nhistory = []\nfor i in range(rounds):\n    print('Start Round {} ...'.format(i + 1))\n    curr_parameters = global_net.get_parameters()\n    new_parameters = dict([(layer_name, {'weight': 0, 'bias': 0}) for layer_name in curr_parameters])\n    for client in clients:\n        client_parameters = client.train(curr_parameters)\n        fraction = client.get_dataset_size() / total_train_size\n        for layer_name in client_parameters:\n            new_parameters[layer_name]['weight'] += fraction * client_parameters[layer_name]['weight']\n            new_parameters[layer_name]['bias'] += fraction * client_parameters[layer_name]['bias']\n    global_net.apply_parameters(new_parameters)\n    \n    train_loss, train_acc = global_net.evaluate(train_dataset)\n    dev_loss, dev_acc = global_net.evaluate(dev_dataset)\n    print('After round {}, train_loss = {}, dev_loss = {}, dev_acc = {}\\n'.format(i + 1, round(train_loss, 4), \n            round(dev_loss, 4), round(dev_acc, 4)))\n    history.append((train_loss, dev_loss))","execution_count":12,"outputs":[{"output_type":"stream","text":"Start Round 1 ...\nclient_0: Loss = 0.438, Accuracy = 0.8659\nclient_1: Loss = 0.444, Accuracy = 0.8705\nclient_2: Loss = 0.4676, Accuracy = 0.8555\nclient_3: Loss = 0.4682, Accuracy = 0.8606\nclient_4: Loss = 0.4482, Accuracy = 0.8613\nclient_5: Loss = 0.4662, Accuracy = 0.8558\nclient_6: Loss = 0.477, Accuracy = 0.8529\nclient_7: Loss = 0.4952, Accuracy = 0.8454\nAfter round 1, train_loss = 0.3834, dev_loss = 0.3865, dev_acc = 0.8877\n\nStart Round 2 ...\nclient_0: Loss = 0.2844, Accuracy = 0.9168\nclient_1: Loss = 0.3033, Accuracy = 0.911\nclient_2: Loss = 0.3172, Accuracy = 0.9043\nclient_3: Loss = 0.3112, Accuracy = 0.911\nclient_4: Loss = 0.2966, Accuracy = 0.9105\nclient_5: Loss = 0.313, Accuracy = 0.9083\nclient_6: Loss = 0.3124, Accuracy = 0.909\nclient_7: Loss = 0.3161, Accuracy = 0.9056\nAfter round 2, train_loss = 0.2818, dev_loss = 0.2831, dev_acc = 0.9186\n\nStart Round 3 ...\nclient_0: Loss = 0.2087, Accuracy = 0.9379\nclient_1: Loss = 0.2272, Accuracy = 0.9348\nclient_2: Loss = 0.2359, Accuracy = 0.9281\nclient_3: Loss = 0.2343, Accuracy = 0.9323\nclient_4: Loss = 0.2179, Accuracy = 0.9359\nclient_5: Loss = 0.2323, Accuracy = 0.9332\nclient_6: Loss = 0.2336, Accuracy = 0.9318\nclient_7: Loss = 0.2299, Accuracy = 0.9375\nAfter round 3, train_loss = 0.2159, dev_loss = 0.216, dev_acc = 0.9393\n\nStart Round 4 ...\nclient_0: Loss = 0.1639, Accuracy = 0.9533\nclient_1: Loss = 0.1792, Accuracy = 0.9505\nclient_2: Loss = 0.1853, Accuracy = 0.9426\nclient_3: Loss = 0.1876, Accuracy = 0.9451\nclient_4: Loss = 0.1711, Accuracy = 0.9503\nclient_5: Loss = 0.1862, Accuracy = 0.9495\nclient_6: Loss = 0.1805, Accuracy = 0.9477\nclient_7: Loss = 0.1844, Accuracy = 0.9464\nAfter round 4, train_loss = 0.1735, dev_loss = 0.1743, dev_acc = 0.9494\n\nStart Round 5 ...\nclient_0: Loss = 0.1364, Accuracy = 0.9579\nclient_1: Loss = 0.1503, Accuracy = 0.957\nclient_2: Loss = 0.1524, Accuracy = 0.9549\nclient_3: Loss = 0.1553, Accuracy = 0.9564\nclient_4: Loss = 0.1391, Accuracy = 0.9586\nclient_5: Loss = 0.154, Accuracy = 0.96\nclient_6: Loss = 0.1526, Accuracy = 0.9549\nclient_7: Loss = 0.153, Accuracy = 0.9587\nAfter round 5, train_loss = 0.1448, dev_loss = 0.1459, dev_acc = 0.9586\n\nStart Round 6 ...\nclient_0: Loss = 0.1115, Accuracy = 0.9676\nclient_1: Loss = 0.1299, Accuracy = 0.9612\nclient_2: Loss = 0.1298, Accuracy = 0.961\nclient_3: Loss = 0.1359, Accuracy = 0.9605\nclient_4: Loss = 0.1199, Accuracy = 0.9633\nclient_5: Loss = 0.1324, Accuracy = 0.9646\nclient_6: Loss = 0.1303, Accuracy = 0.9613\nclient_7: Loss = 0.134, Accuracy = 0.9643\nAfter round 6, train_loss = 0.1264, dev_loss = 0.1264, dev_acc = 0.9649\n\nStart Round 7 ...\nclient_0: Loss = 0.0973, Accuracy = 0.9712\nclient_1: Loss = 0.116, Accuracy = 0.9648\nclient_2: Loss = 0.1156, Accuracy = 0.9669\nclient_3: Loss = 0.119, Accuracy = 0.9674\nclient_4: Loss = 0.1053, Accuracy = 0.9675\nclient_5: Loss = 0.117, Accuracy = 0.9689\nclient_6: Loss = 0.1141, Accuracy = 0.9662\nclient_7: Loss = 0.1207, Accuracy = 0.966\nAfter round 7, train_loss = 0.1126, dev_loss = 0.1155, dev_acc = 0.9673\n\nStart Round 8 ...\nclient_0: Loss = 0.0889, Accuracy = 0.9718\nclient_1: Loss = 0.1031, Accuracy = 0.9703\nclient_2: Loss = 0.1033, Accuracy = 0.9699\nclient_3: Loss = 0.1109, Accuracy = 0.9694\nclient_4: Loss = 0.0938, Accuracy = 0.9718\nclient_5: Loss = 0.1065, Accuracy = 0.9705\nclient_6: Loss = 0.1038, Accuracy = 0.9711\nclient_7: Loss = 0.1093, Accuracy = 0.9697\nAfter round 8, train_loss = 0.101, dev_loss = 0.1031, dev_acc = 0.971\n\nStart Round 9 ...\nclient_0: Loss = 0.0796, Accuracy = 0.975\nclient_1: Loss = 0.0944, Accuracy = 0.9712\nclient_2: Loss = 0.0933, Accuracy = 0.9733\nclient_3: Loss = 0.1012, Accuracy = 0.9709\nclient_4: Loss = 0.0864, Accuracy = 0.9745\nclient_5: Loss = 0.0963, Accuracy = 0.9738\nclient_6: Loss = 0.0926, Accuracy = 0.9716\nclient_7: Loss = 0.1007, Accuracy = 0.9712\nAfter round 9, train_loss = 0.093, dev_loss = 0.0966, dev_acc = 0.9739\n\nStart Round 10 ...\nclient_0: Loss = 0.0743, Accuracy = 0.9768\nclient_1: Loss = 0.0859, Accuracy = 0.9737\nclient_2: Loss = 0.0867, Accuracy = 0.9744\nclient_3: Loss = 0.0961, Accuracy = 0.9741\nclient_4: Loss = 0.0782, Accuracy = 0.9771\nclient_5: Loss = 0.0879, Accuracy = 0.9759\nclient_6: Loss = 0.0874, Accuracy = 0.9729\nclient_7: Loss = 0.0938, Accuracy = 0.9736\nAfter round 10, train_loss = 0.0869, dev_loss = 0.0913, dev_acc = 0.9743\n\nStart Round 11 ...\nclient_0: Loss = 0.0681, Accuracy = 0.9785\nclient_1: Loss = 0.0804, Accuracy = 0.9747\nclient_2: Loss = 0.0816, Accuracy = 0.9764\nclient_3: Loss = 0.0883, Accuracy = 0.9758\nclient_4: Loss = 0.0741, Accuracy = 0.9769\nclient_5: Loss = 0.0827, Accuracy = 0.9768\nclient_6: Loss = 0.08, Accuracy = 0.9758\nclient_7: Loss = 0.0879, Accuracy = 0.9738\nAfter round 11, train_loss = 0.0813, dev_loss = 0.085, dev_acc = 0.9769\n\nStart Round 12 ...\nclient_0: Loss = 0.0619, Accuracy = 0.9831\nclient_1: Loss = 0.0769, Accuracy = 0.9772\nclient_2: Loss = 0.0746, Accuracy = 0.9777\nclient_3: Loss = 0.0856, Accuracy = 0.9772\nclient_4: Loss = 0.0686, Accuracy = 0.9787\nclient_5: Loss = 0.0769, Accuracy = 0.9797\nclient_6: Loss = 0.0768, Accuracy = 0.9762\nclient_7: Loss = 0.0826, Accuracy = 0.9763\nAfter round 12, train_loss = 0.0767, dev_loss = 0.0821, dev_acc = 0.9783\n\nStart Round 13 ...\nclient_0: Loss = 0.0591, Accuracy = 0.9828\nclient_1: Loss = 0.0752, Accuracy = 0.9769\nclient_2: Loss = 0.0708, Accuracy = 0.9781\nclient_3: Loss = 0.08, Accuracy = 0.979\nclient_4: Loss = 0.064, Accuracy = 0.98\nclient_5: Loss = 0.0714, Accuracy = 0.9793\nclient_6: Loss = 0.0733, Accuracy = 0.9796\nclient_7: Loss = 0.079, Accuracy = 0.9765\nAfter round 13, train_loss = 0.0741, dev_loss = 0.0803, dev_acc = 0.9789\n\nStart Round 14 ...\nclient_0: Loss = 0.0543, Accuracy = 0.9841\nclient_1: Loss = 0.0687, Accuracy = 0.9792\nclient_2: Loss = 0.0656, Accuracy = 0.9828\nclient_3: Loss = 0.0753, Accuracy = 0.9811\nclient_4: Loss = 0.0608, Accuracy = 0.9814\nclient_5: Loss = 0.0694, Accuracy = 0.9803\nclient_6: Loss = 0.0682, Accuracy = 0.9782\nclient_7: Loss = 0.0762, Accuracy = 0.9776\nAfter round 14, train_loss = 0.0695, dev_loss = 0.0763, dev_acc = 0.9785\n\nStart Round 15 ...\nclient_0: Loss = 0.0523, Accuracy = 0.9848\nclient_1: Loss = 0.0644, Accuracy = 0.9819\nclient_2: Loss = 0.0657, Accuracy = 0.9794\nclient_3: Loss = 0.0739, Accuracy = 0.981\nclient_4: Loss = 0.057, Accuracy = 0.9828\nclient_5: Loss = 0.0648, Accuracy = 0.9841\nclient_6: Loss = 0.0653, Accuracy = 0.9793\nclient_7: Loss = 0.0731, Accuracy = 0.9779\nAfter round 15, train_loss = 0.0672, dev_loss = 0.0738, dev_acc = 0.9787\n\nStart Round 16 ...\nclient_0: Loss = 0.05, Accuracy = 0.9852\nclient_1: Loss = 0.064, Accuracy = 0.9813\nclient_2: Loss = 0.0598, Accuracy = 0.9839\nclient_3: Loss = 0.0721, Accuracy = 0.9813\nclient_4: Loss = 0.0552, Accuracy = 0.9825\nclient_5: Loss = 0.0619, Accuracy = 0.9842\nclient_6: Loss = 0.0631, Accuracy = 0.9817\nclient_7: Loss = 0.071, Accuracy = 0.9788\nAfter round 16, train_loss = 0.0641, dev_loss = 0.0709, dev_acc = 0.9798\n\nStart Round 17 ...\nclient_0: Loss = 0.0483, Accuracy = 0.986\nclient_1: Loss = 0.0594, Accuracy = 0.9817\nclient_2: Loss = 0.0556, Accuracy = 0.9849\nclient_3: Loss = 0.0688, Accuracy = 0.9807\nclient_4: Loss = 0.0545, Accuracy = 0.9828\nclient_5: Loss = 0.0596, Accuracy = 0.9838\nclient_6: Loss = 0.0582, Accuracy = 0.9832\nclient_7: Loss = 0.0665, Accuracy = 0.9801\nAfter round 17, train_loss = 0.0613, dev_loss = 0.0697, dev_acc = 0.9802\n\nStart Round 18 ...\nclient_0: Loss = 0.0457, Accuracy = 0.9871\nclient_1: Loss = 0.0606, Accuracy = 0.9815\nclient_2: Loss = 0.0555, Accuracy = 0.9846\nclient_3: Loss = 0.0655, Accuracy = 0.9816\nclient_4: Loss = 0.0519, Accuracy = 0.9853\nclient_5: Loss = 0.0573, Accuracy = 0.9836\nclient_6: Loss = 0.0562, Accuracy = 0.9818\nclient_7: Loss = 0.0671, Accuracy = 0.9795\nAfter round 18, train_loss = 0.0604, dev_loss = 0.0682, dev_acc = 0.9805\n\nStart Round 19 ...\nclient_0: Loss = 0.0439, Accuracy = 0.9879\nclient_1: Loss = 0.0559, Accuracy = 0.9837\nclient_2: Loss = 0.0528, Accuracy = 0.9855\nclient_3: Loss = 0.0643, Accuracy = 0.9827\nclient_4: Loss = 0.0498, Accuracy = 0.9846\nclient_5: Loss = 0.0551, Accuracy = 0.9859\nclient_6: Loss = 0.0541, Accuracy = 0.9834\nclient_7: Loss = 0.064, Accuracy = 0.9814\nAfter round 19, train_loss = 0.0576, dev_loss = 0.0665, dev_acc = 0.9811\n\nStart Round 20 ...\n","name":"stdout"},{"output_type":"stream","text":"client_0: Loss = 0.0419, Accuracy = 0.9873\nclient_1: Loss = 0.0544, Accuracy = 0.9827\nclient_2: Loss = 0.0509, Accuracy = 0.9858\nclient_3: Loss = 0.0611, Accuracy = 0.9828\nclient_4: Loss = 0.047, Accuracy = 0.9863\nclient_5: Loss = 0.0532, Accuracy = 0.9864\nclient_6: Loss = 0.0529, Accuracy = 0.9825\nclient_7: Loss = 0.0612, Accuracy = 0.9832\nAfter round 20, train_loss = 0.0553, dev_loss = 0.0651, dev_acc = 0.9818\n\nStart Round 21 ...\nclient_0: Loss = 0.04, Accuracy = 0.9886\nclient_1: Loss = 0.0532, Accuracy = 0.9828\nclient_2: Loss = 0.0474, Accuracy = 0.9855\nclient_3: Loss = 0.0591, Accuracy = 0.9834\nclient_4: Loss = 0.0464, Accuracy = 0.986\nclient_5: Loss = 0.0509, Accuracy = 0.9864\nclient_6: Loss = 0.051, Accuracy = 0.9831\nclient_7: Loss = 0.0595, Accuracy = 0.9821\nAfter round 21, train_loss = 0.054, dev_loss = 0.0648, dev_acc = 0.9811\n\nStart Round 22 ...\nclient_0: Loss = 0.0394, Accuracy = 0.9889\nclient_1: Loss = 0.052, Accuracy = 0.9838\nclient_2: Loss = 0.0465, Accuracy = 0.9868\nclient_3: Loss = 0.0575, Accuracy = 0.9845\nclient_4: Loss = 0.0447, Accuracy = 0.9863\nclient_5: Loss = 0.0495, Accuracy = 0.9866\nclient_6: Loss = 0.0509, Accuracy = 0.9843\nclient_7: Loss = 0.0563, Accuracy = 0.9829\nAfter round 22, train_loss = 0.0519, dev_loss = 0.0628, dev_acc = 0.9817\n\nStart Round 23 ...\nclient_0: Loss = 0.0374, Accuracy = 0.9906\nclient_1: Loss = 0.0482, Accuracy = 0.9849\nclient_2: Loss = 0.0449, Accuracy = 0.9878\nclient_3: Loss = 0.0559, Accuracy = 0.9828\nclient_4: Loss = 0.0417, Accuracy = 0.9879\nclient_5: Loss = 0.0483, Accuracy = 0.9876\nclient_6: Loss = 0.0482, Accuracy = 0.9852\nclient_7: Loss = 0.055, Accuracy = 0.9837\nAfter round 23, train_loss = 0.0511, dev_loss = 0.0628, dev_acc = 0.9814\n\nStart Round 24 ...\nclient_0: Loss = 0.0363, Accuracy = 0.9895\nclient_1: Loss = 0.0481, Accuracy = 0.9846\nclient_2: Loss = 0.0435, Accuracy = 0.9888\nclient_3: Loss = 0.0541, Accuracy = 0.9857\nclient_4: Loss = 0.0404, Accuracy = 0.989\nclient_5: Loss = 0.0469, Accuracy = 0.9877\nclient_6: Loss = 0.0448, Accuracy = 0.9861\nclient_7: Loss = 0.0536, Accuracy = 0.9849\nAfter round 24, train_loss = 0.0492, dev_loss = 0.0611, dev_acc = 0.9828\n\nStart Round 25 ...\nclient_0: Loss = 0.0354, Accuracy = 0.9899\nclient_1: Loss = 0.0459, Accuracy = 0.9855\nclient_2: Loss = 0.0427, Accuracy = 0.9887\nclient_3: Loss = 0.0532, Accuracy = 0.9843\nclient_4: Loss = 0.0398, Accuracy = 0.9884\nclient_5: Loss = 0.046, Accuracy = 0.9873\nclient_6: Loss = 0.0432, Accuracy = 0.9873\nclient_7: Loss = 0.0518, Accuracy = 0.9842\nAfter round 25, train_loss = 0.048, dev_loss = 0.0602, dev_acc = 0.9827\n\nStart Round 26 ...\nclient_0: Loss = 0.0355, Accuracy = 0.9889\nclient_1: Loss = 0.0435, Accuracy = 0.9862\nclient_2: Loss = 0.0399, Accuracy = 0.9904\nclient_3: Loss = 0.0511, Accuracy = 0.9845\nclient_4: Loss = 0.039, Accuracy = 0.9891\nclient_5: Loss = 0.0443, Accuracy = 0.988\nclient_6: Loss = 0.0429, Accuracy = 0.9869\nclient_7: Loss = 0.0512, Accuracy = 0.9843\nAfter round 26, train_loss = 0.0469, dev_loss = 0.0603, dev_acc = 0.9826\n\nStart Round 27 ...\nclient_0: Loss = 0.0333, Accuracy = 0.9912\nclient_1: Loss = 0.0456, Accuracy = 0.985\nclient_2: Loss = 0.0397, Accuracy = 0.9891\nclient_3: Loss = 0.0505, Accuracy = 0.9851\nclient_4: Loss = 0.0373, Accuracy = 0.989\nclient_5: Loss = 0.0427, Accuracy = 0.9883\nclient_6: Loss = 0.0426, Accuracy = 0.9862\nclient_7: Loss = 0.0501, Accuracy = 0.9859\nAfter round 27, train_loss = 0.0456, dev_loss = 0.059, dev_acc = 0.9828\n\nStart Round 28 ...\nclient_0: Loss = 0.0321, Accuracy = 0.9911\nclient_1: Loss = 0.0423, Accuracy = 0.9856\nclient_2: Loss = 0.0375, Accuracy = 0.9885\nclient_3: Loss = 0.0478, Accuracy = 0.985\nclient_4: Loss = 0.0366, Accuracy = 0.9896\nclient_5: Loss = 0.0426, Accuracy = 0.989\nclient_6: Loss = 0.0412, Accuracy = 0.9873\nclient_7: Loss = 0.0473, Accuracy = 0.9872\nAfter round 28, train_loss = 0.0446, dev_loss = 0.0584, dev_acc = 0.9841\n\nStart Round 29 ...\nclient_0: Loss = 0.0309, Accuracy = 0.9921\nclient_1: Loss = 0.0424, Accuracy = 0.9872\nclient_2: Loss = 0.0371, Accuracy = 0.9895\nclient_3: Loss = 0.0475, Accuracy = 0.9862\nclient_4: Loss = 0.036, Accuracy = 0.99\nclient_5: Loss = 0.0402, Accuracy = 0.9892\nclient_6: Loss = 0.0398, Accuracy = 0.9872\nclient_7: Loss = 0.0481, Accuracy = 0.9872\nAfter round 29, train_loss = 0.0438, dev_loss = 0.0577, dev_acc = 0.9835\n\nStart Round 30 ...\nclient_0: Loss = 0.0322, Accuracy = 0.9903\nclient_1: Loss = 0.0411, Accuracy = 0.9877\nclient_2: Loss = 0.0364, Accuracy = 0.9907\nclient_3: Loss = 0.046, Accuracy = 0.9859\nclient_4: Loss = 0.0344, Accuracy = 0.9902\nclient_5: Loss = 0.0404, Accuracy = 0.9896\nclient_6: Loss = 0.0373, Accuracy = 0.9882\nclient_7: Loss = 0.0461, Accuracy = 0.9873\nAfter round 30, train_loss = 0.0424, dev_loss = 0.0572, dev_acc = 0.9839\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot([i + 1 for i in range(len(history))], [history[i][0] for i in range(len(history))], color='r', label='train loss')\nplt.plot([i + 1 for i in range(len(history))], [history[i][1] for i in range(len(history))], color='b', label='dev loss')\nplt.legend()\nplt.title('Training history')\nplt.show()","execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 360x360 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}